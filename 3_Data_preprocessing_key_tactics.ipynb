{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical varibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common issues with categorical variables:\n",
    "* You can’t fit categorical variables into a regression equation in their raw form. They must be treated.\n",
    "* Most of the algorithms (or ML libraries) produce better result with numerical variable. In python, library “sklearn” requires features in numerical arrays. Look at the below snapshot. I have applied random forest using sklearn library on titanic data set (only two features sex and pclass are taken as independent variables). It has returned an error because feature “sex” is categorical and has not been converted to numerical form.\n",
    "\n",
    "Refer the following blog for more facts about categorical variable:\n",
    "https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some commonly used methods to deal with categorical variables:\n",
    "- Convert into numbers:\n",
    "    - Convert into number: label encoding, one-hot encoding (when it does not makes sense to assign values to different categories)\n",
    "    - Convert numeric bins to numbers, e.g., age group. In this case you can craete a new feature using mean or mode of each age bucket. It would comprise of additional weight for levels. Or you can create two new features, one for lower bound of age and another for upper bound. However, label encoder here does not work well, because these numerical bins will be treated same as multiple levels of non-numeric feature, and wouldn't provide any additional information\n",
    "- Combine levels: \n",
    "    - Use business logic: e.g., combine levels of a variable 'zip code' at state or district level\n",
    "    - Use frequency distribution or respsonse rate: e.g., combine levels with frequency less than 5% of total observation, or with similar response rate, into same group.\n",
    "- Dummy coding:same as one-hot encoding, a duplicate variable which represents one level of a categorical variable. Presence of a level is represent by 1 and absence is represented by 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some commonly used methods to deal with continuous variables:\n",
    "- Binning the variable: easier to discover set of patterns in continous variables, however leads to loss of informatio and power. So it is advisable to create small bins initially. It's also important to consider distribution of data prior to deciding bin size.\n",
    "- Normalization: to compare variables at a 'neutral' or 'standard' scale. Commonly used in algorithms such as k-means, clustering, etc.\n",
    "- Transformations for skewed distribution: e.g., log, sqrt, box-cox, power, etc. (https://www.statisticshowto.com/box-cox-transformation/)\n",
    "- Use of business logic: \n",
    "- New features: identify the hidden patterns in data and create new variables.\n",
    "- Treating outliers: outliers is an abnormal value which stands apart from rest of data points. (1) crate a box plot. You’ll get Q1, Q2 and Q3. (data points > Q3 + 1.5IQR) and (data points < Q1 – 1.5IQR) will be considered as outliers. IQR is Interquartile Range. IQR = Q3-Q1. (2) considering the scope of analysis, you can remove the top 1% adn bottom 1% of values.\n",
    "- Principal component analysis: if too many variables: (1) time consuming, (2) lots of noise, (3) may tell similar information. PCA helps to reduce noise, redundancy and enables quick computations.In PCA, components are represented by PC1 or Comp 1, PC2 or Comp 2.. and so on. Here, PC1 will have highest variance followed by PC2, PC3 and so on. Our motive should be to select components with eigen values greater than 1. Eigen values are represented by ‘Standard Deviation’.\n",
    "- Factor analysis: variable reduction. Group variables that are highly correlated together. (1) EFA (Exploratory Factor Analysis) - it identifies and summarizes the underlying correlation structure in a data set; (2) CFA (Confirmatory Factor Analysis) - it attempts to confirm hypothesis using hte correlation structure and rate 'goodness of fit'.\n",
    "\n",
    "Date & Time variable\n",
    "- Create new variables: Date, Month, Year, Time, Days of Month, Days of Week, Days of Year\n",
    "- Create bins: month into quarter/half year\n",
    "- Convert date to numbers:This will allow you to analyze dates using various statistical techniques such as correlation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to deal with imbalanced data\n",
    "1. Choose a good metrics: <br\\>Confusion Matrix: recision/Recall/F1-score\n",
    "2. Resampling Your Dataset:\n",
    "  * You can add copies of instances from the under-represented class called over-sampling<br\\>\n",
    "  * use systematic algorithms to generate new instances: Synthetic Minority Over-sampling Technique<br\\>\n",
    "  * You can delete instances from the over-represented class, called under-sampling<br\\>\n",
    "  * imbalanced-learn package: https://github.com/scikit-learn-contrib/imbalanced-learn\n",
    "3. Penalized Models:<br\\>\n",
    "比如在RandomForestClassifier里面调节class_weight={0:1,1:4}，LogisticRegression里调节 penalty, penalized-SVM\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset:<br/>\n",
    "http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
